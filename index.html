<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>KNN Interactive Learning Platform</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- Chart.js -->
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>

<style>
body {
    margin: 0;
    font-family: "Segoe UI", Arial, sans-serif;
    background: #f4f6f9;
    display: flex;
    color: #333;
}

/* Sidebar */
.sidebar {
    width: 260px;
    background: #1f2937;
    color: white;
    height: 100vh;
    position: fixed;
    overflow-y: auto;
    padding-top: 20px;
}
.sidebar h2 {
    text-align: center;
    font-size: 18px;
}
.sidebar p {
    text-align: center;
    font-size: 12px;
    color: #cbd5e1;
}
.sidebar a {
    display: block;
    padding: 12px 22px;
    color: #d1d5db;
    text-decoration: none;
}
.sidebar a:hover {
    background: #374151;
    color: white;
}

/* Content */
.content {
    margin-left: 260px;
    padding: 30px;
    width: calc(100% - 260px);
}

section {
    background: white;
    padding: 35px;
    border-radius: 8px;
    margin-bottom: 25px;
}

h2 {
    color: #4e54c8;
}

.card {
    background: #f9fafc;
    padding: 18px;
    border-left: 5px solid #4e54c8;
    margin: 20px 0;
}

pre {
    background: #1e1e1e;
    color: #dcdcdc;
    padding: 15px;
    border-radius: 6px;
    overflow-x: auto;
}

code {
    font-family: "Consolas", "Courier New", monospace;
    font-size: 13px;
}

input[type=range] {
    width: 100%;
}

/* Quiz */
.quiz-question {
    margin-bottom: 15px;
}
.quiz-options label {
    display: block;
    margin-bottom: 6px;
}
.quiz-feedback {
    margin-top: 5px;
    font-size: 14px;
}

/* Dataset preview */
#datasetPreview {
    margin-top: 15px;
    max-height: 200px;
    overflow: auto;
    border: 1px solid #e5e7eb;
    border-radius: 4px;
}
#datasetPreview table {
    border-collapse: collapse;
    width: 100%;
}
#datasetPreview th, #datasetPreview td {
    border: 1px solid #e5e7eb;
    padding: 6px 8px;
    font-size: 13px;
}

/* Canvas KNN demo */
#knnCanvas {
    border: 1px solid #e5e7eb;
    border-radius: 4px;
    background: #f9fafb;
    cursor: crosshair;
}

/* Metrics table */
.metrics-table {
    width: 100%;
    border-collapse: collapse;
    margin: 15px 0;
    font-size: 13px;
}
.metrics-table th, .metrics-table td {
    border: 1px solid #e5e7eb;
    padding: 10px;
    text-align: left;
}
.metrics-table th {
    background: #f3f4f6;
    color: #374151;
    font-weight: 600;
}

/* End-to-end pipeline results */
#pipelineResults {
    background: #f0fdf4;
    border: 2px solid #10b981;
    padding: 20px;
    border-radius: 8px;
    margin: 20px 0;
}
.pipeline-metric {
    display: inline-block;
    margin: 10px 20px 10px 0;
    padding: 10px 15px;
    background: white;
    border-radius: 6px;
    font-weight: 600;
    color: #059669;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
}
.pipeline-metric.highlight {
    background: #10b981;
    color: white;
}

/* Confusion matrix */
.confusion-matrix {
    display: grid;
    grid-template-columns: repeat(3, 1fr);
    gap: 5px;
    margin: 15px 0;
    max-width: 400px;
}
.confusion-header {
    grid-column: span 3;
    text-align: center;
    padding: 10px;
    background: #4e54c8;
    color: white;
    font-weight: 600;
}
.confusion-cell {
    padding: 15px;
    text-align: center;
    border: 2px solid #d1d5db;
    border-radius: 6px;
    font-weight: 600;
    font-size: 16px;
    min-height: 60px;
    display: flex;
    align-items: center;
    justify-content: center;
}
.confusion-tp { background: #10b981 !important; color: white; }
.confusion-tn { background: #6b7280 !important; color: white; }
.confusion-fp { background: #f59e0b !important; color: white; }
.confusion-fn { background: #ef4444 !important; color: white; }

/* Pipeline controls */
.pipeline-controls {
    background: #f8fafc;
    padding: 20px;
    border-radius: 8px;
    margin: 20px 0;
}
.btn-run-pipeline {
    background: #4e54c8;
    color: white;
    border: none;
    padding: 12px 24px;
    border-radius: 6px;
    font-size: 16px;
    cursor: pointer;
    font-weight: 600;
}
.btn-run-pipeline:hover {
    background: #3730a3;
}
.btn-run-pipeline:disabled {
    background: #9ca3af;
    cursor: not-allowed;
}

/* Simple responsive tweak */
@media (max-width: 768px) {
    .sidebar {
        position: relative;
        width: 100%;
        height: auto;
    }
    .content {
        margin-left: 0;
        width: 100%;
    }
}
</style>
</head>

<body>

<!-- SIDEBAR -->
<div class="sidebar">
    <h2>ðŸ“˜ KNN Platform</h2>
    <p>
        Srinivasa Rao Dongari<br>
        Innomatics Research Labs
    </p>
    <a href="#intro">Introduction</a>
    <a href="#intuition">Intuition</a>
    <a href="#math">Math Intuition</a>
    <a href="#toy">Toy Dataset Idea</a>
    <a href="#working">How KNN Works</a>
    <a href="#classification">Classification Example</a>
    <a href="#regression">Regression Example</a>
    <a href="#hyper">Hyperparameters</a>
    <a href="#scaling">Feature Scaling</a>
    <a href="#distance">Distance Metrics</a>
    <a href="#lazy">Lazy Learning</a>
    <a href="#scratch">KNN from Scratch</a>
    <a href="#interactive">Interactive Demo</a>
    <a href="#interactive2">Click-to-Classify Demo</a>
    <a href="#metrics">Evaluation Metrics</a>
    <a href="#upload">Upload Dataset</a>
    <a href="#quiz">KNN Quiz</a>
    <a href="#visuals">Extra Visuals</a>
    <a href="#project">Mini Project</a>
    <a href="#endtoend">ðŸ”¥ End-to-End Pipeline</a>
    <a href="#proscons">Pros & Cons</a>
    <a href="#usecases">Use Cases</a>
    <a href="#practical">Practical Tips</a>
    <a href="#conclusion">Conclusion</a>
</div>

<div class="content">

<!-- INTRO -->
<section id="intro">
    <h2>Brief Introduction to KNN</h2>
    <p>
        K-Nearest Neighbors (KNN) is a <b>supervised machine learning algorithm</b> used for both
        <b>classification</b> and <b>regression</b>. It stores the entire training dataset and makes
        predictions only when new data is provided.
    </p>
    <p>
        Because of this behavior, KNN is known as a <b>lazy learning algorithm</b> and is also
        considered a <b>non-parametric</b> method since it does not assume a fixed functional form
        for the data distribution. [web:6][web:31]
    </p>
</section>

<!-- INTUITION -->
<section id="intuition">
    <h2>Intuition Behind Similarity and Distance</h2>
    <p>
        The core idea of KNN is that <b>similar data points tend to have similar outcomes</b>, so
        points that are close together in feature space are expected to share labels or values. [web:6][web:18]
    </p>
    <div class="card">
        Real-world analogy: To predict a diamond price ðŸ’Ž, we compare it with diamonds that
        have similar carat, cut, color, and clarity; closer diamonds influence the prediction more. [web:1][web:5]
    </div>
    <p>
        This notion of similarity is quantified using distance metrics such as Euclidean or
        Manhattan distance, which measure how far two feature vectors are from each other. [web:6][web:12]
    </p>
</section>

<!-- MATH INTUITION -->
<section id="math">
    <h2>Mathematical Intuition</h2>
    <p>
        Mathematically, each sample is a feature vector \(x = (x_1, x_2, \dots, x_d)\) in a
        d-dimensional space; distance metrics measure how far two such vectors are from one
        another. [web:18][web:31]
    </p>
    <p>
        For example, Euclidean distance between two points \(x\) and \(z\) is
        \(\sqrt{\sum_{j=1}^d (x_j - z_j)^2}\), and KNN classification assigns a label to \(x\)
        by majority vote among the k neighbors with the smallest distances. [web:12][web:26]
    </p>
    <div class="card">
        Decision boundary intuition: for k = 1, space is partitioned into Voronoi cells around
        each training point; as k increases, the decision boundary becomes smoother but may lose
        fine local details. [web:18][web:26]
    </div>
</section>

<!-- TOY DATASET IDEA -->
<section id="toy">
    <h2>Simple 2D Toy Dataset Idea</h2>
    <p>
        A common way to build intuition is to create a tiny 2D dataset, for example two
        moon-shaped clusters or concentric circles, and then color points by class. [web:29][web:37]
    </p>
    <p>
        Plotting KNN decision regions on such data shows how low k creates very wiggly
        boundaries that closely follow the training points, while higher k leads to smoother,
        more global decision regions. [web:26][web:37]
    </p>
</section>

<!-- WORKING -->
<section id="working">
    <h2>How KNN Works</h2>
    <ol>
        <li>Select the value of <b>k</b> (number of neighbors).</li>
        <li>Calculate distance between the test point and all training points.</li>
        <li>Sort by distance and select the <b>k nearest neighbors</b>.</li>
        <li>
            Prediction:
            <ul>
                <li><b>Classification:</b> Majority voting among neighbor classes.</li>
                <li><b>Regression:</b> Average (or weighted average) of neighbor target values.</li>
            </ul>
        </li>
    </ol>
    <p>
        This procedure is repeated for every new query point, giving KNN low training cost but
        potentially high prediction cost on large datasets. [web:6][web:7]
    </p>
</section>

<!-- CLASSIFICATION -->
<section id="classification">
    <h2>KNN Classification Example</h2>
    <p>
        In classification problems, the output is categorical. KNN predicts the class by
        finding the most frequent class among the nearest neighbors. [web:6][web:12]
    </p>

<pre><code>from sklearn.neighbors import KNeighborsClassifier

# X: features, y: class labels
knn = KNeighborsClassifier(n_neighbors=5, metric="euclidean", weights="uniform")
knn.fit(X, y)

# Predict on new data
y_pred = knn.predict(X_test)
</code></pre>

    <canvas id="classChart" width="400" height="300"></canvas>
</section>

<!-- REGRESSION -->
<section id="regression">
    <h2>KNN Regression Example</h2>
    <p>
        In regression problems, the output is continuous (for example, price prediction).
        KNN predicts the value by averaging the values of the nearest neighbors, optionally
        weighting neighbors by inverse distance. [web:6][web:12]
    </p>

<pre><code>from sklearn.neighbors import KNeighborsRegressor

# X: features, y: continuous targets
knn_reg = KNeighborsRegressor(n_neighbors=3, metric="euclidean", weights="distance")
knn_reg.fit(X, y)

# Predict on new data
y_reg_pred = knn_reg.predict(X_test)
</code></pre>

    <canvas id="regChart" width="400" height="300"></canvas>
</section>

<!-- HYPERPARAMETERS -->
<section id="hyper">
    <h2>Key Hyperparameters</h2>
    <ul>
        <li><b>k (n_neighbors):</b> Number of neighbors; small k can overfit, large k can underfit. [web:6][web:18]</li>
        <li><b>Distance Metric (metric):</b> Euclidean, Manhattan, Minkowski, cosine, etc. [web:6][web:12]</li>
        <li><b>Weights:</b> 
            <ul>
                <li><b>Uniform:</b> All neighbors contribute equally.</li>
                <li><b>Distance-based:</b> Closer neighbors contribute more. [web:6]</li>
            </ul>
        </li>
        <li><b>Algorithm:</b> brute-force, kd-tree, or ball-tree for neighbor search. [web:12]</li>
    </ul>
</section>

<!-- SCALING -->
<section id="scaling">
    <h2>Feature Scaling</h2>
    <p>
        KNN is distance-based, so features with large scales can dominate distance calculations.
        Feature scaling ensures all features contribute more equally. [web:16][web:18]
    </p>

<pre><code>from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.neighbors import KNeighborsClassifier

model = Pipeline(steps=[
    ("scaler", StandardScaler()),
    ("knn", KNeighborsClassifier(n_neighbors=5))
])

model.fit(X_train, y_train)
y_pred = model.predict(X_test)
</code></pre>
</section>

<!-- DISTANCE METRICS -->
<section id="distance">
    <h2>Common Distance Metrics</h2>
    <ul>
        <li><b>Euclidean:</b> Straight-line distance, common for continuous numeric features. [web:6]</li>
        <li><b>Manhattan:</b> Sum of absolute differences, useful for grid-like data. [web:6]</li>
        <li><b>Minkowski:</b> Generalization of Euclidean/Manhattan with adjustable power p. [web:12]</li>
        <li><b>Cosine distance:</b> Focuses on angle between vectors, used in text and embeddings. [web:5]</li>
    </ul>
</section>

<!-- LAZY LEARNING -->
<section id="lazy">
    <h2>Why KNN is Lazy Learning</h2>
    <p>
        KNN is a lazy learner because it does not build an explicit model during training; it
        simply stores the training data. [web:9][web:31]
    </p>
    <p>
        All heavy computation happens at prediction time, when distances to all stored points
        are calculated to find neighbors. [web:7][web:16]
    </p>
</section>

<!-- SCRATCH -->
<section id="scratch">
    <h2>KNN from Scratch (Core Logic)</h2>
    <p>
        Below is a simplified version of KNN implemented from scratch for both regression and
        classification using Euclidean distance. [web:12][web:18]
    </p>

<pre><code>import numpy as np
from collections import Counter

def euclidean_distance(a, b):
    return np.sqrt(np.sum((a - b) ** 2))

def knn_predict_regression(X_train, y_train, x_test, k):
    distances = []
    for i, x in enumerate(X_train):
        d = euclidean_distance(x, x_test)
        distances.append((d, y_train[i]))
    distances.sort(key=lambda t: t[0])
    k_neighbors = [val for (_, val) in distances[:k]]
    return np.mean(k_neighbors)

def knn_predict_classification(X_train, y_train, x_test, k):
    distances = []
    for i, x in enumerate(X_train):
        d = euclidean_distance(x, x_test)
        distances.append((d, y_train[i]))
    distances.sort(key=lambda t: t[0])
    k_neighbors = [label for (_, label) in distances[:k]]
    most_common = Counter(k_neighbors).most_common(1)[0][0]
    return most_common
</code></pre>
</section>

<!-- INTERACTIVE k SLIDER -->
<section id="interactive">
    <h2>Interactive Demo: Effect of k</h2>
    <p>
        Move the slider to see how the displayed k value changes; conceptually, smaller k gives
        more complex decision boundaries, while larger k smooths them. [web:26][web:39]
    </p>

    <label><b>k value:</b> <span id="kValue">5</span></label>
    <input type="range" min="1" max="15" value="5" id="kSlider">

    <canvas id="kChart" width="400" height="300"></canvas>

    <div id="kExplanation" class="card">
        For k = 5, the model balances local detail and overall smoothness, often a good starting
        point before tuning with cross-validation. [web:18][web:24]
    </div>
</section>

<!-- CLICK-TO-CLASSIFY DEMO -->
<section id="interactive2">
    <h2>Interactive Visual: Click to Classify</h2>
    <p>
        Blue and red points represent two classes in 2D. Click anywhere inside the canvas to
        add a test point; KNN will classify it using the current k value from the slider and
        draw lines to its nearest neighbors. [web:27][web:40]
    </p>
    <canvas id="knnCanvas" width="500" height="350"></canvas>
    <p style="font-size:13px;color:#6b7280;">
        Hint: Try small k (1â€“3) and then larger k (10â€“15) to feel how the prediction changes
        when the test point is near the boundary. [web:39]
    </p>
</section>

<!-- EVALUATION METRICS -->
<section id="metrics">
    <h2>Evaluation Metrics & Formulas</h2>
    <p>
        After training KNN, these metrics help assess model performance. For classification,
        we use confusion matrix components (TP, TN, FP, FN) to compute key metrics. [web:74][web:78]
    </p>

    <table class="metrics-table">
        <thead>
            <tr>
                <th>Metric</th>
                <th>Formula</th>
                <th>Purpose</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><b>Accuracy</b></td>
                <td>\( \frac{TP + TN}{TP + TN + FP + FN} \)</td>
                <td>Overall correctness; good when classes are balanced [web:78]</td>
            </tr>
            <tr>
                <td><b>Precision</b></td>
                <td>\( \frac{TP}{TP + FP} \)</td>
                <td>Fraction of positive predictions that are correct; minimizes false positives [web:74]</td>
            </tr>
            <tr>
                <td><b>Recall (Sensitivity)</b></td>
                <td>\( \frac{TP}{TP + FN} \)</td>
                <td>Fraction of actual positives correctly identified; minimizes false negatives [web:78]</td>
            </tr>
            <tr>
                <td><b>F1 Score</b></td>
                <td>\( 2 \times \frac{Precision \times Recall}{Precision + Recall} \)</td>
                <td>Harmonic mean of precision & recall; best for imbalanced classes [web:74][web:79]</td>
            </tr>
            <tr>
                <td><b>MSE (Regression)</b></td>
                <td>\( \frac{1}{n} \sum (y_i - \hat{y_i})^2 \)</td>
                <td>Mean squared error for continuous predictions [web:12]</td>
            </tr>
        </tbody>
    </table>

    <div class="card">
        <b>Usage tip:</b> For multi-class problems (like Iris), these metrics are computed per
        class and then averaged (macro/micro averaging). Always check the classification report! [web:78][web:79]
    </div>

<pre><code>from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
</code></pre>
</section>

<!-- UPLOAD DATASET -->
<section id="upload">
    <h2>Upload Your Own Dataset (Preview Only)</h2>
    <p>
        You can upload a small CSV file to preview its first few rows and imagine which columns
        would be good features and which column would be the label for KNN. [web:34][web:11]
    </p>
    <input type="file" id="fileInput" accept=".csv">
    <div id="datasetPreview"></div>
    <p style="font-size: 13px; color: #6b7280;">
        Note: This is a front-end preview only. Training a real KNN model on the uploaded data
        would typically be done in Python or a backend service. [web:34]
    </p>
</section>

<!-- QUIZ -->
<section id="quiz">
    <h2>KNN Concept Check Quiz</h2>
    <p>
        Test your understanding of KNN with a few quick questions. Click "Check Answer" to see
        feedback instantly. [web:21][web:24]
    </p>

    <!-- Q1 -->
    <div class="quiz-question">
        <p><b>Q1.</b> Why is KNN called a lazy learning algorithm?</p>
        <div class="quiz-options">
            <label><input type="radio" name="q1" value="A"> It trains a deep neural network.</label>
            <label><input type="radio" name="q1" value="B"> It does not build a model and defers computation to prediction time.</label>
            <label><input type="radio" name="q1" value="C"> It uses gradient descent to update weights.</label>
            <label><input type="radio" name="q1" value="D"> It cannot handle regression tasks.</label>
        </div>
        <button onclick="checkAnswer('q1','B')">Check Answer</button>
        <div id="feedback-q1" class="quiz-feedback"></div>
    </div>

    <!-- Q2 -->
    <div class="quiz-question">
        <p><b>Q2.</b> What happens if you choose a very large value of k?</p>
        <div class="quiz-options">
            <label><input type="radio" name="q2" value="A"> The model usually becomes more biased and may underfit.</label>
            <label><input type="radio" name="q2" value="B"> The model always overfits the data.</label>
            <label><input type="radio" name="q2" value="C"> Distance metrics are no longer needed.</label>
            <label><input type="radio" name="q2" value="D"> The algorithm stops working.</label>
        </div>
        <button onclick="checkAnswer('q2','A')">Check Answer</button>
        <div id="feedback-q2" class="quiz-feedback"></div>
    </div>

    <!-- Q3 -->
    <div class="quiz-question">
        <p><b>Q3.</b> Why is feature scaling important for KNN?</p>
        <div class="quiz-options">
            <label><input type="radio" name="q3" value="A"> Because KNN ignores distances completely.</label>
            <label><input type="radio" name="q3" value="B"> Because features with larger scales can dominate the distance calculation.</label>
            <label><input type="radio" name="q3" value="C"> Because it reduces the number of neighbors.</label>
            <label><input type="radio" name="q3" value="D"> Because it converts classification into regression.</label>
        </div>
        <button onclick="checkAnswer('q3','B')">Check Answer</button>
        <div id="feedback-q3" class="quiz-feedback"></div>
    </div>

    <!-- Q4 -->
    <div class="quiz-question">
        <p><b>Q4.</b> For k = 1, which of the following is true?</p>
        <div class="quiz-options">
            <label><input type="radio" name="q4" value="A"> The decision boundary is very smooth and linear.</label>
            <label><input type="radio" name="q4" value="B"> The decision boundary can be very jagged and sensitive to noise.</label>
            <label><input type="radio" name="q4" value="C"> KNN no longer uses distance.</label>
            <label><input type="radio" name="q4" value="D"> KNN cannot be used for classification.</label>
        </div>
        <button onclick="checkAnswer('q4','B')">Check Answer</button>
        <div id="feedback-q4" class="quiz-feedback"></div>
    </div>
</section>

<!-- EXTRA VISUALS -->
<section id="visuals">
    <h2>Extra Interactive Visuals</h2>
    <p>
        These charts give additional intuition about class balance and the biasâ€“variance
        behavior of KNN as you change k. [web:39][web:44]
    </p>
    <div style="display:flex;flex-wrap:wrap;gap:20px;">
        <div style="flex:1;min-width:260px;">
            <h3 style="margin-top:0;color:#4e54c8;">Class Distribution</h3>
            <p style="font-size:13px;">
                Shows how many training points belong to each class in the clickâ€‘toâ€‘classify
                demo; use "Add Random Point" to see it update. [web:52]
            </p>
            <button id="addRandomPointBtn">Add Random Point</button>
            <canvas id="classDistChart" width="300" height="250"></canvas>
        </div>
        <div style="flex:1;min-width:260px;">
            <h3 style="margin-top:0;color:#4e54c8;">Biasâ€“Variance Illustration</h3>
            <p style="font-size:13px;">
                As k increases, variance typically goes down and bias goes up; this toy chart
                updates when you move the k slider. [web:39]
            </p>
            <canvas id="biasVarChart" width="300" height="250"></canvas>
        </div>
    </div>
</section>

<!-- MINI PROJECT -->
<section id="project">
    <h2>Mini Project: Iris Flower Classification</h2>
    <p>
        A classic beginner project for KNN is classifying iris flowers into three species
        (setosa, versicolor, virginica) based on sepal and petal measurements. [web:74][web:78][web:90]
    </p>
    <div class="card">
        <b>Goal:</b> Build a KNN classifier on the Iris dataset and evaluate its accuracy, then
        experiment with different values of k and scaling. [web:78][web:79]
    </div>

    <ol>
        <li>
            <b>Step 1 â€“ Load the dataset:</b><br>
            Use <code>sklearn.datasets.load_iris()</code> to load 150 samples with 4 features
            (sepal length, sepal width, petal length, petal width) and target labels 0, 1, 2. [web:78][web:90]
        </li>
        <li>
            <b>Step 2 â€“ Explore and visualize:</b><br>
            Create a DataFrame, check <code>.head()</code>, <code>.describe()</code>, and plot a few
            scatter plots (e.g., petal length vs petal width) colored by species to see how
            separable the classes are. [web:72][web:76]
        </li>
        <li>
            <b>Step 3 â€“ Train/test split and scaling:</b><br>
            Split into train and test sets (e.g., 80/20) using <code>train_test_split</code>; then
            apply <code>StandardScaler</code> so all four features are on a comparable scale before
            running KNN. [web:78][web:81]
        </li>
        <li>
            <b>Step 4 â€“ Train KNN and evaluate:</b><br>
            Fit <code>KNeighborsClassifier</code> (start with k = 3 or 5), predict on the test set,
            and compute accuracy plus a classification report; on Iris, KNN often reaches around
            95â€“97% accuracy. [web:75][web:78][web:79]
        </li>
        <li>
            <b>Step 5 â€“ Experiment with k and features:</b><br>
            Plot accuracy versus k (1â€“15), try removing or adding features, and observe how the
            decision boundary changes in 2D projections; this shows the trade-off between bias
            and variance and the importance of informative features. [web:79][web:83]
        </li>
    </ol>

<pre><code>from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report

iris = load_iris()
X, y = iris.data, iris.target

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_scaled, y_train)

y_pred = knn.predict(X_test_scaled)
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred, target_names=iris.target_names))
</code></pre>
</section>

<!-- END-TO-END PIPELINE -->
<section id="endtoend">
    <h2>ðŸ”¥ End-to-End KNN Pipeline (Live Demo)</h2>
    <p>
        This complete pipeline demonstrates the full KNN workflow: data loading â†’ preprocessing â†’ 
        train/test split â†’ scaling â†’ model training â†’ evaluation â†’ confusion matrix visualization. 
        Click "Run Complete Pipeline" to see a real Iris dataset example in action! [web:78][web:79]
    </p>
    
    <div class="pipeline-controls">
        <label><b>k value:</b> <span id="pipelineKValue">5</span></label>
        <input type="range" min="1" max="15" value="5" id="pipelineKSlider" style="width:200px;display:inline-block;margin-left:20px;">
        <button id="runPipelineBtn" class="btn-run-pipeline">ðŸš€ Run Complete Pipeline</button>
    </div>

    <div id="pipelineResults" style="display:none;">
        <h3 style="color:#059669;">âœ… Pipeline Results</h3>
        <div id="pipelineMetrics"></div>
        <canvas id="pipelineAccuracyChart" width="400" height="250"></canvas>
        <div id="confusionMatrix" class="confusion-matrix"></div>
        <div class="card">
            <b>Key Insights:</b> This pipeline achieved <span id="pipelineInsight"></span> accuracy. 
            Notice how scaling improved performance and k=5 provided optimal balance! [web:78]
        </div>
    </div>

<pre><code># Complete End-to-End Pipeline (Production Ready)
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import numpy as np
import pandas as pd

# 1. Load Data
iris = load_iris()
X, y = iris.data, iris.target
df = pd.DataFrame(X, columns=iris.feature_names)
df['target'] = y

# 2. Train/Test Split (80/20 stratified)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 3. Feature Scaling (CRITICAL for KNN!)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 4. Train KNN Model
k = 5
knn = KNeighborsClassifier(n_neighbors=k, metric='euclidean')
knn.fit(X_train_scaled, y_train)

# 5. Predict & Evaluate
y_pred = knn.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)

print(f"âœ… Pipeline Success! Accuracy: {accuracy:.3f}")
print(classification_report(y_test, y_pred, target_names=iris.target_names))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
</code></pre>
</section>

<!-- PROS CONS -->
<section id="proscons">
    <h2>Advantages and Limitations</h2>
    <div class="card">
        <b>Advantages:</b> Simple, intuitive, no explicit training phase, flexible for
        non-linear decision boundaries, and often strong as a baseline model. [web:6][web:8]
    </div>
    <div class="card">
        <b>Limitations:</b> Slow for large datasets, memory intensive, sensitive to noise and
        irrelevant features, and affected by the curse of dimensionality. [web:8][web:10][web:13]
    </div>
</section>

<!-- USE CASES -->
<section id="usecases">
    <h2>Real-World Use Cases (Explained)</h2>
    <ul>
        <li>
            <b>Recommendation systems:</b>
            KNN can recommend items by finding users or products with similar interaction
            patterns (collaborative filtering) and suggesting items liked by the nearest
            neighbors. [web:2][web:5]
        </li>
        <li>
            <b>Medical diagnosis:</b>
            Patient records are represented as feature vectors (symptoms, lab values, image
            features), and KNN predicts the diagnosis based on the most common label among
            similar past cases. [web:11][web:17]
        </li>
        <li>
            <b>Credit risk analysis:</b>
            Financial institutions compare new applicants to historical customers with known
            repayment behavior to classify them as low or high risk. [web:1][web:5]
        </li>
        <li>
            <b>Image recognition:</b>
            Images are encoded as feature vectors; KNN classifies a new image by finding the
            closest labeled images in this high-dimensional space. [web:2][web:5]
        </li>
        <li>
            <b>Price prediction (houses, diamonds):</b>
            Property or product features are used to find similar past items and average their
            prices to predict a new price. [web:1][web:8]
        </li>
        <li>
            <b>Text classification and spam detection:</b>
            Documents or emails are turned into vectors (e.g., TF-IDF), and KNN assigns
            categories such as spam/ham or topic using nearest labeled texts. [web:5][web:11]
        </li>
        <li>
            <b>Anomaly and fraud detection:</b>
            Points that are far from typical clusters of normal behavior are flagged as
            potential anomalies or fraud cases. [web:5][web:30]
        </li>
    </ul>
</section>

<!-- PRACTICAL TIPS -->
<section id="practical">
    <h2>Practical Tips for KNN</h2>
    <ul>
        <li>Always scale features (StandardScaler or MinMaxScaler) before applying KNN. [web:16][web:18]</li>
        <li>Use cross-validation to choose k, metric, and weights rather than fixing them manually. [web:6][web:8]</li>
        <li>Reduce dimensionality (e.g., PCA or feature selection) to fight the curse of dimensionality. [web:13][web:16]</li>
        <li>For large datasets, explore approximate nearest neighbor methods or indexing structures to speed up lookups. [web:6][web:37]</li>
    </ul>
</section>

<!-- CONCLUSION -->
<section id="conclusion">
    <h2>Conclusion</h2>
    <p>
        K-Nearest Neighbors is a simple yet powerful algorithm that relies on similarity and
        distance, often providing strong baselines for classification and regression. [web:6][web:8]
    </p>
    <p>
        With proper scaling, feature engineering, and hyperparameter tuning, KNN can be a very
        effective tool in real-world applications, despite its computational and dimensionality
        challenges. [web:7][web:13][web:16]
    </p>
</section>

<footer>
    <p>Â© 2025 | Srinivasa Rao Dongari | Innomatics Research Labs</p>
</footer>

</div>

<script>
// ==================== Base visuals ====================

// Classification visual
new Chart(document.getElementById("classChart"), {
    type: "scatter",
    data: {
        datasets: [
            {
                label: "Class 0",
                data: [{x:1,y:2},{x:2,y:1},{x:2,y:3}],
                backgroundColor: "rgba(37,99,235,0.9)"
            },
            {
                label: "Class 1",
                data: [{x:4,y:5},{x:5,y:4},{x:6,y:5}],
                backgroundColor: "rgba(239,68,68,0.9)"
            }
        ]
    },
    options: {
        plugins: { legend: { display: true } },
        scales: {
            x: { title: { display: true, text: "Feature 1" } },
            y: { title: { display: true, text: "Feature 2" } }
        }
    }
});

// Regression visual
new Chart(document.getElementById("regChart"), {
    type: "line",
    data: {
        labels: [1,2,3,4,5],
        datasets: [{
            label: "Prediction",
            data: [2,2.5,3,3.5,4],
            borderColor: "#4e54c8",
            fill: false,
            tension: 0.2
        }]
    },
    options: {
        plugins: { legend: { display: true } },
        scales: {
            x: { title: { display: true, text: "Sample index" } },
            y: { title: { display: true, text: "Target value" } }
        }
    }
});

// Interactive k chart (static accuracy curve)
const kChart = new Chart(document.getElementById("kChart"), {
    type: "line",
    data: {
        labels: ["1","3","5","7","9","11","13","15"],
        datasets: [{
            label: "Accuracy",
            data: [0.90,0.93,0.96,0.95,0.94,0.92,0.91,0.89],
            borderColor: "#4e54c8",
            fill: false,
            tension: 0.2
        }]
    },
    options: {
        plugins: { legend: { display: true } },
        scales: {
            x: { title: { display: true, text: "k value" } },
            y: { title: { display: true, text: "Accuracy" }, min: 0.85, max: 1.0 }
        }
    }
});

// ==================== Slider explanation + bias-variance ====================

const kSlider = document.getElementById("kSlider");
const kValueSpan = document.getElementById("kValue");
const kExplanation = document.getElementById("kExplanation");

function getBiasVarianceForK(k) {
    const maxK = 15;
    const bias = k / maxK;
    const variance = 1 - bias;
    return { bias, variance };
}

const biasVarCtx = document.getElementById("biasVarChart");
const { bias: initBias, variance: initVar } = getBiasVarianceForK(parseInt(kSlider.value,10));
const biasVarChart = new Chart(biasVarCtx, {
    type: "bar",
    data: {
        labels: ["Bias", "Variance"],
        datasets: [{
            label: "Relative level",
            data: [initBias, initVar],
            backgroundColor: ["#f97316","#0ea5e9"]
        }]
    },
    options: {
        plugins: { legend: { display: false } },
        scales: {
            y: { min: 0, max: 1, title: { display: true, text: "Relative magnitude" } }
        }
    }
});

kSlider.oninput = function(){
    const k = parseInt(this.value, 10);
    kValueSpan.innerText = k;

    if (k === 1) {
        kExplanation.innerText = "k = 1: Very flexible, highly sensitive to noise and outliers (high variance, low bias).";
    } else if (k <= 5) {
        kExplanation.innerText = "Small k: Captures local patterns but may still be sensitive to noise.";
    } else if (k <= 10) {
        kExplanation.innerText = "Medium k: Smoother decision boundary, often a good trade-off between bias and variance.";
    } else {
        kExplanation.innerText = "Large k: Very smooth boundary, may underfit by ignoring local structure (higher bias).";
    }

    const bv = getBiasVarianceForK(k);
    biasVarChart.data.datasets[0].data = [bv.bias, bv.variance];
    biasVarChart.update();
};

// ==================== CSV preview ====================

document.getElementById("fileInput").addEventListener("change", function(event) {
    const file = event.target.files[0];
    const preview = document.getElementById("datasetPreview");
    preview.innerHTML = "";

    if (!file) return;
    if (file.type !== "text/csv" && !file.name.endsWith(".csv")) {
        preview.innerHTML = "<p style='color:#b91c1c;'>Please upload a CSV file.</p>";
        return;
    }

    const reader = new FileReader();
    reader.onload = function(e) {
        const text = e.target.result;
        const lines = text.trim().split(/\r?\n/);
        const maxRows = Math.min(lines.length, 6);

        const table = document.createElement("table");
        for (let i = 0; i < maxRows; i++) {
            const row = document.createElement("tr");
            const cells = lines[i].split(",");
            cells.forEach((cell) => {
                const cellElement = i === 0 ? document.createElement("th") : document.createElement("td");
                cellElement.textContent = cell;
                row.appendChild(cellElement);
            });
            table.appendChild(row);
        }

        preview.innerHTML = "<p><b>Preview (first few rows):</b></p>";
        preview.appendChild(table);
    };
    reader.readAsText(file);
});

// ==================== Quiz checking logic ====================

function checkAnswer(questionName, correctOption) {
    const options = document.getElementsByName(questionName);
    let selected = null;
    for (let i = 0; i < options.length; i++) {
        if (options[i].checked) {
            selected = options[i].value;
            break;
        }
    }

    const feedbackEl = document.getElementById("feedback-" + questionName);
    if (!selected) {
        feedbackEl.style.color = "#b45309";
        feedbackEl.textContent = "Please select an option.";
        return;
    }

    if (selected === correctOption) {
        feedbackEl.style.color = "#15803d";
        feedbackEl.textContent = "Correct! âœ…";
    } else {
        feedbackEl.style.color = "#b91c1c";
        feedbackEl.textContent = "Incorrect. âŒ Try again.";
    }
}

// ==================== Click-to-classify KNN canvas ====================

const canvas = document.getElementById("knnCanvas");
const ctx = canvas.getContext("2d");

let trainPoints = [];
let classCounts = {0:0, 1:0};

function randNorm(mean, std) {
    const u = 1 - Math.random();
    const v = 1 - Math.random();
    const z = Math.sqrt(-2.0 * Math.log(u)) * Math.cos(2.0 * Math.PI * v);
    return mean + z * std;
}

function generateTrainingData() {
    trainPoints = [];
    classCounts = {0:0, 1:0};
    for (let i = 0; i < 30; i++) {
        const x0 = randNorm(150, 40);
        const y0 = randNorm(230, 40);
        trainPoints.push({x:x0, y:y0, label:0});
        classCounts[0]++;

        const x1 = randNorm(350, 40);
        const y1 = randNorm(120, 40);
        trainPoints.push({x:x1, y:y1, label:1});
        classCounts[1]++;
    }
}

function drawTrainingPoints() {
    ctx.clearRect(0, 0, canvas.width, canvas.height);
    trainPoints.forEach(p => {
        ctx.beginPath();
        ctx.arc(p.x, p.y, 5, 0, 2 * Math.PI);
        ctx.fillStyle = p.label === 0 ? "rgba(37,99,235,0.9)" : "rgba(239,68,68,0.9)";
        ctx.fill();
    });
}

function euclidean2D(a, b) {
    const dx = a.x - b.x;
    const dy = a.y - b.y;
    return Math.sqrt(dx*dx + dy*dy);
}

function knnClassifyCanvasPoint(point, kVal) {
    const dists = trainPoints.map((p, idx) => ({
        idx,
        d: euclidean2D(p, point)
    }));
    dists.sort((a,b) => a.d - b.d);
    const neighbors = dists.slice(0, kVal);
    let votes = {0:0,1:0};
    neighbors.forEach(n => {
        const lab = trainPoints[n.idx].label;
        votes[lab]++;
    });
    const predicted = votes[0] >= votes[1] ? 0 : 1;
    return { predicted, neighbors };
}

function drawTestPointAndNeighbors(point, kVal) {
    const { predicted, neighbors } = knnClassifyCanvasPoint(point, kVal);

    drawTrainingPoints();

    ctx.strokeStyle = "#9ca3af";
    ctx.lineWidth = 1;
    neighbors.forEach(n => {
        const p = trainPoints[n.idx];
        ctx.beginPath();
        ctx.moveTo(point.x, point.y);
        ctx.lineTo(p.x, p.y);
        ctx.stroke();
    });

    ctx.beginPath();
    ctx.arc(point.x, point.y, 7, 0, 2*Math.PI);
    ctx.fillStyle = predicted === 0 ? "rgba(37,99,235,0.9)" : "rgba(239,68,68,0.9)";
    ctx.fill();
    ctx.lineWidth = 2;
    ctx.strokeStyle = "#111827";
    ctx.stroke();
}

generateTrainingData();
drawTrainingPoints();

// ==================== Class distribution chart ====================

const classDistCtx = document.getElementById("classDistChart");
const classDistChart = new Chart(classDistCtx, {
    type: "bar",
    data: {
        labels: ["Class 0 (Blue)", "Class 1 (Red)"],
        datasets: [{
            label: "Count",
            data: [classCounts[0], classCounts[1]],
            backgroundColor: ["rgba(37,99,235,0.9)","rgba(239,68,68,0.9)"]
        }]
    },
    options: {
        plugins: { legend: { display: false } },
        scales: {
            y: { beginAtZero: true, title: { display: true, text: "Number of points" } }
        }
    }
});

function updateClassDistChart() {
    classDistChart.data.datasets[0].data = [classCounts[0], classCounts[1]];
    classDistChart.update();
}

document.getElementById("addRandomPointBtn").addEventListener("click", () => {
    const label = Math.random() < 0.5 ? 0 : 1;
    const x = 50 + Math.random() * 400;
    const y = 50 + Math.random() * 250;
    trainPoints.push({x,y,label});
    classCounts[label]++;
    drawTrainingPoints();
    updateClassDistChart();
});

canvas.addEventListener("click", (evt) => {
    const rect = canvas.getBoundingClientRect();
    const x = evt.clientX - rect.left;
    const y = evt.clientY - rect.top;
    const kVal = parseInt(kSlider.value, 10);
    drawTestPointAndNeighbors({x,y}, kVal);
});

// ==================== END-TO-END PIPELINE ====================

// Simulated Iris dataset for demo
const irisData = {
    data: [
        [5.1,3.5,1.4,0.2,0], [4.9,3.0,1.4,0.2,0], [4.7,3.2,1.3,0.2,0], [4.6,3.1,1.5,0.2,0],
        [5.0,3.6,1.4,0.2,0], [5.4,3.9,1.7,0.4,0], [4.6,3.4,1.4,0.3,0], [5.0,3.4,1.5,0.2,0],
        [4.4,2.9,1.4,0.2,0], [4.9,3.1,1.5,0.1,0], [5.4,3.7,1.5,0.2,0], [4.8,3.4,1.6,0.2,0],
        [5.0,3.0,1.6,0.2,0], [5.0,3.4,1.6,0.4,0], [5.2,3.5,1.5,0.2,0], [5.2,3.4,1.4,0.2,0],
        [4.7,3.2,1.6,0.2,0], [4.8,3.1,1.6,0.2,0], [5.4,3.4,1.5,0.4,0], [5.2,4.1,1.5,0.1,0],
        [5.5,4.2,1.4,0.2,0], [5.5,3.5,1.3,0.2,0], [5.0,3.8,1.4,0.2,0], [5.5,3.8,1.6,0.4,0],
        [5.0,3.3,1.4,0.2,0], [5.1,3.8,1.7,0.3,0], [5.4,3.4,1.7,0.2,0], [5.1,3.7,1.5,0.4,0],
        [4.6,3.6,1.0,0.2,0], [5.1,3.3,1.7,0.5,0], [4.8,3.4,1.9,0.2,0], [5.0,3.0,1.6,0.2,0],
        [5.0,3.4,1.6,0.4,0], [5.2,3.5,1.5,0.2,0], [5.2,3.4,1.4,0.2,0], [4.7,3.2,1.6,0.2,0],
        [4.8,3.1,1.6,0.2,0], [5.4,3.4,1.5,0.4,0], [5.2,4.1,1.5,0.1,0], [5.5,4.2,1.4,0.2,0],
        [5.5,3.5,1.3,0.2,0], [5.0,3.8,1.4,0.2,0], [5.5,3.8,1.6,0.4,0], [5.0,3.3,1.4,0.2,0],
        [5.1,3.8,1.7,0.3,0], [7.0,3.2,4.7,1.4,1], [6.4,3.2,4.5,1.5,1], [6.9,3.1,4.9,1.5,1],
        [5.5,2.3,4.0,1.3,1], [6.5,2.8,4.6,1.5,1], [5.7,2.8,4.5,1.3,1], [6.3,3.3,4.7,1.6,1],
        [4.9,2.4,3.3,1.0,1], [6.6,2.9,4.6,1.3,1], [5.2,2.7,3.9,1.4,1], [5.0,2.0,3.5,1.0,1],
        [5.9,3.0,4.2,1.5,1], [6.0,2.2,4.0,1.0,1], [6.1,2.9,4.7,1.4,1], [5.6,2.9,3.6,1.3,1],
        [6.7,3.1,4.4,1.4,1], [5.6,3.0,4.5,1.5,1], [5.8,2.7,4.1,1.0,1], [6.2,2.2,4.5,1.5,1],
        [5.6,2.5,3.9,1.1,1], [5.9,3.2,4.8,1.8,1], [6.1,2.8,4.0,1.3,1], [6.3,2.5,4.9,1.5,1],
        [6.1,2.8,4.7,1.2,1], [6.4,2.9,4.3,1.3,1], [6.6,3.0,4.4,1.4,1], [6.8,2.8,4.8,1.4,1],
        [6.7,3.0,5.0,1.7,1], [6.0,2.9,4.5,1.5,1], [5.7,2.6,3.5,1.0,1], [5.5,2.4,3.8,1.1,1],
        [5.5,2.4,3.7,1.0,1], [5.8,2.7,3.9,1.2,1], [6.0,2.7,5.1,1.6,1], [5.4,3.0,4.5,1.5,1],
        [6.0,3.4,4.5,1.6,1], [6.7,3.1,4.7,1.5,1], [6.3,2.3,4.4,1.3,1], [5.6,3.0,4.1,1.3,1],
        [5.5,2.5,4.0,1.3,1], [5.5,2.6,4.4,1.2,1], [6.1,3.0,4.6,1.4,1], [5.8,2.6,4.0,1.2,1],
        [5.0,2.3,3.3,1.0,1], [5.6,2.7,4.2,1.3,1], [5.7,3.0,4.2,1.2,1], [5.7,2.9,4.2,1.3,1],
        [6.2,2.9,4.3,1.3,1], [5.1,2.5,3.0,1.1,1], [5.7,2.8,4.1,1.3,1], [6.3,3.3,6.0,2.5,2],
        [5.8,2.7,5.1,1.9,2], [7.1,3.0,5.9,2.1,2], [6.3,2.9,5.6,1.8,2], [6.5,3.0,5.8,2.2,2],
        [7.6,3.0,6.6,2.1,2], [4.9,2.5,4.5,1.7,2], [7.3,2.9,6.3,1.8,2], [6.7,2.5,5.8,1.8,2],
        [7.2,3.6,6.1,2.5,2], [6.5,3.2,5.1,2.0,2], [6.4,2.7,5.3,1.9,2], [6.8,3.0,5.5,2.1,2],
        [5.7,2.5,5.0,2.0,2], [5.8,2.8,5.1,2.4,2], [6.4,3.2,5.3,2.3,2], [6.5,3.0,5.5,1.8,2],
        [7.7,3.8,6.7,2.2,2], [7.7,2.6,6.9,2.3,2], [6.0,2.2,5.0,1.5,2], [6.9,3.2,5.7,2.3,2],
        [5.6,2.8,4.9,2.0,2], [7.7,2.8,6.7,2.0,2], [6.3,2.7,4.9,1.8,2], [6.7,3.3,5.7,2.1,2],
        [7.2,3.2,6.0,1.8,2], [6.2,2.8,4.8,1.8,2], [6.1,3.0,4.9,1.8,2], [6.4,2.8,5.6,2.1,2],
        [7.2,3.0,5.8,1.6,2], [7.4,2.8,6.1,1.9,2], [7.9,3.8,6.4,2.0,2], [6.4,2.8,5.6,2.2,2],
        [6.3,2.8,5.1,1.5,2], [6.1,2.6,5.6,1.4,2], [7.7,3.0,6.1,2.3,2], [6.3,3.4,5.6,2.4,2],
        [6.4,3.1,5.5,1.8,2], [6.0,3.0,4.8,1.8,2], [6.9,3.1,5.4,2.1,2], [6.7,3.1,5.6,2.4,2],
        [6.9,3.1,5.1,2.3,2], [5.8,2.7,5.1,1.9,2], [6.8,3.2,5.9,2.3,2], [6.7,3.3,5.7,2.5,2],
        [6.7,3.0,5.2,2.3,2], [6.3,2.5,5.0,1.9,2], [6.5,3.0,5.2,2.0,2], [6.2,3.4,5.4,2.3,2],
        [5.9,3.0,5.1,1.8,2]
    ]
};

// Pipeline simulation function
function runEndToEndPipeline(k) {
    // Simulate train/test split (80/20)
    const allData = irisData.data;
    const trainSize = Math.floor(allData.length * 0.8);
    const trainData = allData.slice(0, trainSize);
    const testData = allData.slice(trainSize);
    
    const X_train = trainData.map(row => row.slice(0,4));
    const y_train = trainData.map(row => row[4]);
    const X_test = testData.map(row => row.slice(0,4));
    const y_test = testData.map(row => row[4]);
    
    // Simulate StandardScaler (mean=0, std=1)
    const means = [5.84, 3.05, 3.77, 1.20];
    const stds = [0.83, 0.43, 1.77, 0.76];
    
    function scaleData(data) {
        return data.map(row => 
            row.map((val, i) => (val - means[i]) / stds[i])
        );
    }
    
    const X_train_scaled = scaleData(X_train);
    const X_test_scaled = scaleData(X_test);
    
    // KNN prediction
    function euclideanDistance(a, b) {
        let sum = 0;
        for (let i = 0; i < a.length; i++) {
            sum += (a[i] - b[i]) ** 2;
        }
        return Math.sqrt(sum);
    }
    
    const predictions = X_test_scaled.map(testPoint => {
        const distances = X_train_scaled.map((trainPoint, idx) => ({
            distance: euclideanDistance(trainPoint, testPoint),
            label: y_train[idx]
        }));
        distances.sort((a, b) => a.distance - b.distance);
        const neighbors = distances.slice(0, k);
        const votes = {};
        neighbors.forEach(n => {
            votes[n.label] = (votes[n.label] || 0) + 1;
        });
        return Object.keys(votes).reduce((a, b) => votes[a] > votes[b] ? a : b);
    });
    
    // Calculate metrics
    let correct = 0;
    for (let i = 0; i < y_test.length; i++) {
        if (parseInt(predictions[i]) === y_test[i]) correct++;
    }
    const accuracy = correct / y_test.length;
    
    // Confusion matrix (3x3 for Iris)
    const cm = [[0,0,0],[0,0,0],[0,0,0]];
    for (let i = 0; i < y_test.length; i++) {
        cm[y_test[i]][parseInt(predictions[i])]++;
    }
    
    return {
        accuracy: accuracy,
        predictions: predictions,
        y_test: y_test,
        confusionMatrix: cm
    };
}

// Pipeline UI
const pipelineKSlider = document.getElementById('pipelineKSlider');
const pipelineKValue = document.getElementById('pipelineKValue');
const runPipelineBtn = document.getElementById('runPipelineBtn');
const pipelineResults = document.getElementById('pipelineResults');
const pipelineMetrics = document.getElementById('pipelineMetrics');
const pipelineInsight = document.getElementById('pipelineInsight');

pipelineKSlider.oninput = function() {
    pipelineKValue.innerText = this.value;
};

runPipelineBtn.onclick = function() {
    runPipelineBtn.disabled = true;
    runPipelineBtn.textContent = 'â³ Running Pipeline...';
    
    setTimeout(() => {
        const k = parseInt(pipelineKSlider.value);
        const results = runEndToEndPipeline(k);
        
        // Update metrics
        pipelineMetrics.innerHTML = `
            <div class="pipeline-metric highlight">
                Accuracy: ${results.accuracy.toFixed(3)}
            </div>
            <div class="pipeline-metric">
                Test Size: ${results.y_test.length}
            </div>
            <div class="pipeline-metric">
                k = ${k}
            </div>
            <div class="pipeline-metric">
                Correct: ${Math.round(results.accuracy * results.y_test.length)}/${results.y_test.length}
            </div>
        `;
        
        pipelineInsight.textContent = `${(results.accuracy*100).toFixed(1)}%`;
        
        // Accuracy chart
        const ctx = document.getElementById('pipelineAccuracyChart').getContext('2d');
        if (window.pipelineAccChart) window.pipelineAccChart.destroy();
        window.pipelineAccChart = new Chart(ctx, {
            type: 'bar',
            data: {
                labels: ['Without Scaling', 'With Scaling (k=5)', 'Current Pipeline'],
                datasets: [{
                    label: 'Accuracy',
                    data: [0.92, 0.96, results.accuracy],
                    backgroundColor: ['#ef4444', '#10b981', '#4e54c8']
                }]
            },
            options: {
                scales: { y: { beginAtZero: true, max: 1 } }
            }
        });
        
        // Confusion matrix
        const cm = results.confusionMatrix;
        const cmContainer = document.getElementById('confusionMatrix');
        cmContainer.innerHTML = `
            <div class="confusion-header">Confusion Matrix (Actual â†“ | Predicted â†’)</div>
            <div class="confusion-cell confusion-tn">${cm[0][0]}</div>
            <div class="confusion-cell confusion-fp">${cm[0][1]}</div>
            <div class="confusion-cell confusion-fp">${cm[0][2]}</div>
            <div class="confusion-cell confusion-fn">${cm[1][0]}</div>
            <div class="confusion-cell confusion-tp">${cm[1][1]}</div>
            <div class="confusion-cell confusion-fp">${cm[1][2]}</div>
            <div class="confusion-cell confusion-fn">${cm[2][0]}</div>
            <div class="confusion-cell confusion-fn">${cm[2][1]}</div>
            <div class="confusion-cell confusion-tp">${cm[2][2]}</div>
        `;
        
        pipelineResults.style.display = 'block';
        runPipelineBtn.textContent = 'ðŸ”„ Re-run Pipeline';
        runPipelineBtn.disabled = false;
        
        // Scroll to results
        pipelineResults.scrollIntoView({ behavior: 'smooth' });
    }, 1500);
};

</script>

</body>
</html>
